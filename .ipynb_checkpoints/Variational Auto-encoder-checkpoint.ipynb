{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "Based on variational inference\n",
    "\n",
    "$$P(Z|X) = \\frac{P(Z,X)}{P(X)}$$\n",
    "\n",
    "## Some basic information theory\n",
    "### Information \n",
    "$I = -log(P(x))$\n",
    "* Measures the factor of uncertainty removed when x is known\n",
    "* 1 bit can be thought of as information that reduces uncertainty\n",
    "    by a factor of 2\n",
    "E.g. Let's say there is 50% chance of weather being sunny and \n",
    "50% chance of rain tomorrow. When the weather station tells us it is going to be sunny, they have given us 1 bit of information.\n",
    "\n",
    "> Uncertainty reduction is the inverse of the event's probability\n",
    "\n",
    "E.g. If the weather probabilities are sunny 75% and rain 25%,\n",
    "finding out that it is going to be rainy will reduce our \n",
    "uncertainty by $1/0.25 = 4$. This corresponds to $log_2(4) = 2$ bits of information $log_2(\\frac{1}{0.25}) = -log_2(0.25)$\n",
    "\n",
    "### Entropy \n",
    "$H = \\sum_x-log(P(x)) * P(x))$  \n",
    "* Can be thought of as average amount of information relayed by a certain distribution\n",
    "E.g in the above case, the weather station on average transmits\n",
    "$$ H = 0.75 \\times -log(0.75) + 0.25 \\times (-log(0.25)) \n",
    "  = 0.81$$ bits of useful information\n",
    "\n",
    "### Cross-entropy\n",
    "$$H(P,Q) = \\sum_x P(x)(-log(Q(x))$$\n",
    "\n",
    "E.g Lets say we use 2 bits to encode our weather prediction\n",
    "this can be thought of as us predicting the weather to have\n",
    "a 25% chance of either being sunny or rainy\n",
    "The average number of actual bits sent is\n",
    "$H = 0.75 \\times 2 + 0.25 \\times 2 = 2 $bits. If using different number of bits for the different predictions $H = 0.75 \\times 2 + 0.25 \\times 3 = 2.25 $bits \n",
    "\n",
    "Based on the entropy and cross-entropy, we can see that our _predicted_ probability distribution Q(x) differs from the _actual_ probability distribution P(x) by \n",
    "$KL(P||Q) = 2.25 - 0.81 = 1.54 $bits  \n",
    "If predictions are perfect i.e. Q(x) = P(x), then H(P,Q) = H(P)  \n",
    "Therefore, $H(P,Q) = H(P) + KL(P||Q)$  \n",
    "$KL(P||Q)$ means KL-divergence of Q w.r.t P\n",
    "\n",
    "\\begin{align}\n",
    "KL(P||Q) &= H(P,Q) - H(P)\\\\\n",
    "         &= \\sum_x P(x)(-log(Q(x)) -  \\sum_xP(x)(-log(P(x))\\\\\n",
    "         &= \\sum_{x} P(x)(-log(Q(x) - (-log(P(x)))\\\\\n",
    "         &= \\sum_x P(x)(log(P(x)) - log(Q(x)))\\\\\n",
    "         &= \\sum_x P(x)(log(\\frac{P(x)}{Q(x)}))\\\\\n",
    "\\end{align}\n",
    "#### Some properties of KL-divergence\n",
    "1. $KL(P||Q)$ is alwaysgreater than or equal to 0\n",
    "2. $KL(P||Q)$ is not the same as $KL(Q||P)$\n",
    "\n",
    "\n",
    "## Variational Bayes\n",
    "\n",
    "$$P(Z|X) = \\frac{P(Z,X)}{P(X)} = \\frac{P(X|Z)P(Z)}{P(X)}$$\n",
    "\n",
    "We don't know P(X). If we were to compute it,\n",
    "$P(X) = \\int{P(X|Z)P(Z)dZ}$\n",
    "* Intractable in many cases\n",
    "* If distributions are high dimensional, integral is multi-integral\n",
    "\n",
    "Thus, we can try to approximate the distribution. One method to approximate is Monte Carlo method (Gibbs sampling and other sampling methods) which is unbiased with high variance.\n",
    "\n",
    "Another is variational inference which has low variance but is biased\n",
    "\n",
    "1. Approximate P(Z|X) with Q(Z) that is tractable e.g. Gaussian\n",
    "2. Play with the parameters of Q(Z) in a way that it gets close enough to P(Z|X) i.e. minimize $KL(Q(Z)||P(Z|X))$\n",
    "\n",
    "This brings us to the following objective of minimizing \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "KL(Q(Z)||P(Z|X)) &= \\sum_z Q(Z)log(\\frac{Q(Z)}{P(Z|X)})\\\\\n",
    "                 &= - \\sum_zQ(Z) log(\\frac{P(Z|X)}{Q(Z)})\\\\\n",
    "                 &= - \\sum_z Q(Z) log(\\frac{P(X,Z)}{P(X) Q(Z)})\\\\\n",
    "                 &= - \\sum_z Q(Z) (log(\\frac{P(X,Z)}{Q(Z)}) - log(P(X)))\\\\\n",
    "                 &= - \\sum_z Q(Z) log(\\frac{P(X,Z)}{Q(Z)}) + log(P(X))\\\\\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\therefore log(P(X))    &= KL(Q(Z)||P(Z|X)) + \\sum_z Q(Z)log(\\frac{P(X,Z)}{Q(Z)})\\\\\n",
    "                 &= KL(Q(Z)||P(Z|X)) + L\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "As $log(P(X))$ is a constant, to minimize $KL(Q(Z)||P(Z|X))$,\n",
    "we just need to maximize $L$.\n",
    "\n",
    "$$\\because KL(Q(Z)||P(Z|X)) \\geq 0$$,  \n",
    "$$L \\leq P(X)$$ Thus, L is a lower bound of P(X).\n",
    "\n",
    "\\begin{align}\n",
    "L &= \\sum_z Q(Z) log(\\frac{P(X,Z)}{Q(Z)})\\\\\n",
    "  &= \\sum_z Q(Z) log(\\frac{P(X|Z)P(Z)}{Q(Z)})\\\\\n",
    "  &= \\sum_z Q(Z)(log(P(X|Z)) + log(\\frac{P(Z)}{Q(Z)}))\\\\\n",
    "  &= \\sum_z Q(Z) log(P(X|Z))) + \\sum_z Q(Z) log(\\frac{P(Z)}{Q(Z)})\\\\\n",
    "\\end{align}\n",
    "\n",
    "$$\\sum_z Q(Z) log(P(X|Z))) = E_{Q(Z)}P(X|Z)$$\n",
    "$$\\sum_z Q(Z) log(\\frac{P(Z)}{Q(Z)}) = -KL(Q(Z)||P(Z))$$\n",
    "Representing L as an autoencoder\n",
    "\n",
    "X --> Q(Z|X) --> Z --> P(X|Z) --> X'\n",
    "\n",
    "$E_{Q(Z)}P(X|Z)$ term acts as reconstruction error.\n",
    "$P(X|Z)$ is deterministic meaning one input will get the same output all the time. Thus, it can be considered $P(X|X')$.\n",
    "\n",
    "If $P(X|X')$ is gaussian\n",
    "$$P(X|X') = e^{-|X - X'|^2}$$\n",
    "$$log(P(X|X')) = -|X - X'|^2$$ --> L2 loss\n",
    "\n",
    "If Bernoulli distribution, will be similar to cross-entropy\n",
    "\n",
    "So far the network is all deterministic\n",
    "> To make it probabilistic, \n",
    "    encoder should not parametrize Z but instead the parametrize\n",
    "    the distribution that generates Z i.e. $\\mu$ and $\\sigma$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naren/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../CapsNet/train.csv')\n",
    "labels = np.asarray(df.label).reshape(42000,1)\n",
    "train_x = np.asarray(df[df.columns[1:]]).reshape(42000,28*28)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ SIMPLE MODEL ###############################################\n",
    "mu_data, sigma_data = 3., 1.75\n",
    "mu_z, sigma_z = 0., 1.\n",
    "alpha = 0.2\n",
    "np.random.seed(0)\n",
    "data_distribution = np.random.normal(mu_data, sigma_data, (40000, 10))\n",
    "data_distribution /= np.max(data_distribution,axis=1).reshape(40000,1)\n",
    "\n",
    "latent_distribution = np.random.normal(mu_z, sigma_z, (40000, 16))\n",
    "tf.Graph().as_default()\n",
    "X = tf.placeholder(tf.float32, (None, 784))\n",
    "with tf.name_scope('encoder'):\n",
    "    en_1 = tf.layers.Dense(512)(X)\n",
    "    en_1 = tf.maximum(en_1, alpha*en_1)\n",
    "    en_2 = tf.layers.Dense(256)(en_1)\n",
    "    en_2 = tf.maximum(en_2, alpha*en_2)\n",
    "    en_3 = tf.layers.Dense(128)(en_2)\n",
    "    en_3 = tf.maximum(en_3, alpha*en_3)\n",
    "    encoded = tf.layers.Dense(32)(en_3)\n",
    "    #encoded = tf.maximum(encoded, alpha*encoded)\n",
    "log_stdev = encoded[:,0:16]\n",
    "mean = encoded[:,16:32]\n",
    "latent_samples = tf.placeholder(tf.float32, (None,16))\n",
    "Z = latent_samples * tf.exp(log_stdev) + mean\n",
    "with tf.name_scope('decoder'):\n",
    "    de_1 = tf.layers.Dense(128)(Z)\n",
    "    de_1 = tf.maximum(de_1, alpha*de_1)\n",
    "    de_2 = tf.layers.Dense(256)(de_1)\n",
    "    de_2 = tf.maximum(de_2, alpha*de_2)\n",
    "    de_3 = tf.layers.Dense(512)(de_2)\n",
    "    de_3 = tf.maximum(de_3, alpha*de_3)\n",
    "    output = tf.layers.Dense(784,activation=tf.nn.sigmoid)(de_3)\n",
    "recon_loss = tf.reduce_sum((tf.square(X - output)),axis=1)\n",
    "kl_loss = - 0.5 * tf.reduce_sum(1 + log_stdev - tf.square(mean) - tf.square(tf.exp(log_stdev)), axis=1)\n",
    "loss = tf.reduce_mean(recon_loss + kl_loss)\n",
    "adam = tf.train.AdamOptimizer()\n",
    "train = adam.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0\n",
      "Loss: 176.71806\n",
      "Loss: 86.13999\n",
      "Loss: 68.960815\n",
      "Loss: 42.089844\n",
      "Loss: 37.327477\n",
      "Loss: 105.81069\n",
      "Loss: 60.07544\n",
      "Loss: 40.60073\n",
      "Loss: 47.660053\n",
      "Loss: 77.876854\n",
      "Loss: 38.797943\n",
      "Loss: 54.746956\n",
      "Loss: 71.219475\n",
      "Loss: 41.788605\n",
      "Loss: 47.27316\n",
      "Loss: 26.522472\n",
      "Loss: 47.80396\n",
      "Loss: 50.082848\n",
      "Loss: 61.484234\n",
      "Loss: 65.725784\n",
      "Loss: 70.058205\n",
      "Loss: 54.295845\n",
      "Loss: 31.869999\n",
      "Loss: 27.829477\n",
      "Loss: 72.57536\n",
      "Loss: 26.830242\n",
      "Loss: 57.81515\n",
      "Loss: 24.323423\n",
      "Loss: 41.57086\n",
      "Loss: 79.89962\n",
      "Loss: 46.510654\n",
      "Loss: 53.951756\n",
      "Loss: 21.056206\n",
      "Loss: 62.47521\n",
      "Loss: 19.016842\n",
      "Loss: 56.53398\n",
      "Loss: 63.367165\n",
      "Loss: 57.221268\n",
      "Loss: 39.302254\n",
      "Loss: 27.662477\n",
      "Loss: 53.691757\n",
      "Loss: 38.90088\n",
      "Loss: 52.052193\n",
      "Loss: 58.164482\n",
      "Loss: 45.844\n",
      "Loss: 44.99283\n",
      "Loss: 39.269173\n",
      "Loss: 40.07831\n",
      "Loss: 42.23399\n",
      "Loss: 84.642204\n",
      "Loss: 33.225983\n",
      "Loss: 63.901604\n",
      "Loss: 45.78642\n",
      "Loss: 32.828465\n",
      "Loss: 52.030678\n",
      "Loss: 21.493973\n",
      "Loss: 45.933163\n",
      "Loss: 67.14117\n",
      "Loss: 61.419632\n",
      "Loss: 65.387886\n",
      "Loss: 20.618263\n",
      "Loss: 61.233204\n",
      "Loss: 52.767204\n",
      "Loss: 47.53548\n",
      "Loss: 24.049767\n",
      "Loss: 29.478033\n",
      "Loss: 38.492027\n",
      "Loss: 62.23883\n",
      "Loss: 43.13426\n",
      "Loss: 48.237213\n",
      "Loss: 52.271\n",
      "Loss: 54.645638\n",
      "Loss: 61.810486\n",
      "Loss: 38.550114\n",
      "Loss: 31.017591\n",
      "Loss: 38.834236\n",
      "Loss: 45.997177\n",
      "Loss: 52.979614\n",
      "Loss: 62.98253\n",
      "Loss: 53.754414\n",
      "Loss: 37.3198\n",
      "Loss: 48.90781\n",
      "Loss: 61.540012\n",
      "Loss: 35.15349\n",
      "Loss: 43.70277\n",
      "Loss: 33.868927\n",
      "Loss: 40.174248\n",
      "Loss: 42.496872\n",
      "Loss: 41.09687\n",
      "Loss: 34.016453\n",
      "Loss: 32.19044\n",
      "Loss: 42.21112\n",
      "Loss: 50.051933\n",
      "Loss: 53.97928\n",
      "Loss: 45.636623\n",
      "Loss: 50.461437\n",
      "Loss: 39.478424\n",
      "Loss: 29.025656\n",
      "Loss: 41.887646\n",
      "Loss: 32.42014\n",
      "Loss: 38.57247\n",
      "Loss: 49.1542\n",
      "Loss: 47.654945\n",
      "Loss: 36.712204\n",
      "Loss: 31.615828\n",
      "Loss: 53.981182\n",
      "Loss: 33.874092\n",
      "Loss: 66.41932\n",
      "Loss: 24.997673\n",
      "Loss: 56.51106\n",
      "Loss: 54.790657\n",
      "Loss: 50.951775\n",
      "Loss: 35.425484\n",
      "Loss: 45.05654\n",
      "Loss: 20.8241\n",
      "Loss: 50.24955\n",
      "Loss: 38.03126\n",
      "Loss: 69.49755\n",
      "Loss: 65.54019\n",
      "Loss: 47.266476\n",
      "Loss: 38.946312\n",
      "Loss: 15.209268\n",
      "Loss: 20.206354\n",
      "Loss: 59.80301\n",
      "Loss: 45.414246\n",
      "Loss: 57.588406\n",
      "Loss: 36.106964\n",
      "Loss: 17.670565\n",
      "Loss: 32.685272\n",
      "Loss: 52.30791\n",
      "Loss: 40.162094\n",
      "Loss: 38.242935\n",
      "Loss: 66.20047\n",
      "Loss: 49.88613\n",
      "Loss: 38.927364\n",
      "Loss: 36.790154\n",
      "Loss: 49.293407\n",
      "Loss: 55.262722\n",
      "Loss: 73.18185\n",
      "Loss: 51.745293\n",
      "Loss: 93.88069\n",
      "Loss: 49.037235\n",
      "Loss: 47.884903\n",
      "Loss: 50.669994\n",
      "Loss: 43.116474\n",
      "Loss: 42.632393\n",
      "Loss: 45.246143\n",
      "Loss: 20.505184\n",
      "Loss: 34.57638\n",
      "Loss: 52.764664\n",
      "Loss: 26.78316\n",
      "Loss: 22.5349\n",
      "Loss: 15.2312155\n",
      "Loss: 43.647545\n",
      "Loss: 51.93633\n",
      "Loss: 26.933554\n",
      "Loss: 56.585354\n",
      "Loss: 43.39236\n",
      "Loss: 35.27813\n",
      "Loss: 74.045525\n",
      "Loss: 19.326744\n",
      "Loss: 37.134098\n",
      "Loss: 29.15749\n",
      "Loss: 62.747395\n",
      "Loss: 51.859364\n",
      "Loss: 31.862423\n",
      "Loss: 48.68129\n",
      "Loss: 37.8364\n",
      "Loss: 43.89329\n",
      "Loss: 31.445364\n",
      "Loss: 25.038158\n",
      "Loss: 55.596313\n",
      "Loss: 38.713005\n",
      "Loss: 55.042343\n",
      "Loss: 28.918623\n",
      "Loss: 51.378277\n",
      "Loss: 35.61603\n",
      "Loss: 42.826714\n",
      "Loss: 35.294197\n",
      "Loss: 19.663076\n",
      "Loss: 29.798914\n",
      "Loss: 24.54193\n",
      "Loss: 45.174786\n",
      "Loss: 34.936195\n",
      "Loss: 37.51074\n",
      "Loss: 42.805096\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "bs = 10\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess,'./checkpoints/simple.ckpt')\n",
    "    try:\n",
    "        for i in range(epochs):\n",
    "            print('Ep: ' + str(i))\n",
    "            for b in range(40000):\n",
    "            #batch = train_x[b*batch_s:]\n",
    "                sess.run(train,{X:train_x[b*bs:b*bs+1],latent_samples:latent_distribution[b*bs:b*bs+1]})\n",
    "                if b % 10 == 0:\n",
    "                    saver.save(sess,'./checkpoints/simple.ckpt')\n",
    "                    print(\"Loss: \" + str(sess.run(loss,{X:train_x[b*bs:b*bs+1],latent_samples:latent_distribution[b*bs:b*bs+1]})))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        saver.save(sess,'./checkpoints/simple.ckpt')\n",
    "        print('Model saved')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/simple.ckpt\n",
      "[[ 0.49072147  0.27344777  0.36601261  0.8159857   0.46677805  0.35776554\n",
      "   0.54717307  0.4877827   0.2667516   1.        ]\n",
      " [ 0.15562885  0.36178482  0.61852379 -0.16648737  0.54728843  1.\n",
      "   0.9120218   0.78121508 -0.19731969  0.61954722]\n",
      " [ 0.53872489  0.73663366  0.58234488  0.12916687  0.63438279  1.\n",
      "   0.74062651  0.46483566  0.64851545  0.41318566]\n",
      " [ 0.37936773  0.3524125   1.          0.56451019  0.35917102  0.27761476\n",
      "   0.71340978  0.68306209  0.42316618  0.32498537]\n",
      " [ 0.85991503  0.63204934  0.71561783  0.63673528  0.88973944  1.\n",
      "   0.81692877  0.31732474  0.87881902  0.9355185 ]]\n",
      "[[ 0.5648229   0.4378252   0.53567755  0.6407115   0.5929521   0.55325466\n",
      "   0.7483129   0.55739665  0.5691584   0.70712   ]\n",
      " [ 0.5706934   0.564654    0.47659892  0.19214869  0.6301438   0.6107527\n",
      "   0.8660286   0.8391267  -0.31949863  0.39746046]\n",
      " [ 0.5444164   0.7693986   0.44586438  0.582441    0.5344002   0.7162865\n",
      "   0.5137376   0.59873486  0.3964852   0.4981642 ]\n",
      " [ 0.35963482  0.39104378  0.86861587  0.44394377  0.35489625  0.21414177\n",
      "   0.7470394   0.52132773  0.34994996  0.39202276]\n",
      " [ 0.76111484  0.4504186   0.35910392  0.8478953   0.65100724  0.8506854\n",
      "   0.73976     0.46774492  0.7303579   0.94045573]]\n",
      "0.04914755\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess,'./checkpoints/simple.ckpt')\n",
    "    print(data_distribution[0:5])\n",
    "    print(sess.run(output,{X:data_distribution,latent_samples:latent_distribution})[0:5])\n",
    "    print(sess.run(loss,{X:data_distribution,latent_samples:latent_distribution}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### MNIST MODEL ########################################\n",
    "k = 9\n",
    "mu_z, sigma_z = 0., 1.\n",
    "latent_dist = np.random.normal(mu_z,sigma_z,(42000,7,7,32))\n",
    "tf.Graph().as_default()\n",
    "X = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "with tf.name_scope('encoder'):\n",
    "    en1 = tf.layers.Conv2D(64, (k,k))(X)\n",
    "    en1 = tf.nn.leaky_relu(en1)\n",
    "    en2 = tf.layers.Conv2D(64, (7,7))(en1)\n",
    "    en2 = tf.nn.leaky_relu(en2)\n",
    "    encoded = tf.layers.Conv2D(64, (3,3),padding='same',strides=(2,2))(en2)\n",
    "    #encoded = tf.nn.leaky_relu(encoded)\n",
    "batch_size = tf.shape(encoded)[0]\n",
    "latent_samples = tf.placeholder(tf.float32,(None,7,7,32))\n",
    "log_sigma = encoded[:,:,:,0:32]\n",
    "mu = encoded[:,:,:,32:64]\n",
    "Z = log_sigma * latent_samples + mu \n",
    "\n",
    "with tf.name_scope('decoder'):\n",
    "    de1 = tf.layers.Conv2DTranspose(64,(3,3),padding='same',strides=(2,2))(Z)\n",
    "    de1 = tf.nn.leaky_relu(de1)\n",
    "    de2 = tf.layers.Conv2DTranspose(64,(7,7))(de1)\n",
    "    de2 = tf.nn.leaky_relu(de2)\n",
    "    de3 = tf.layers.Conv2DTranspose(64,(9,9))(de2)\n",
    "    de3 = tf.nn.leaky_relu(de3)\n",
    "    output = tf.layers.Conv2DTranspose(1,(3,3),padding='same',activation=tf.nn.sigmoid)(de3)\n",
    "    \n",
    "squared_loss = tf.reduce_sum(tf.)#tf.reduce_sum(tf.square(X - output))\n",
    "kl = 0.5 * tf.reduce_sum(tf.exp(log_sigma) + tf.square(mu) - 1. - log_sigma)\n",
    "mnist_train = tf.train.AdamOptimizer().minimize(squared_loss+kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loss: 125.301704\n",
      "Loss: 87.7269\n",
      "Loss: 110.86874\n",
      "Loss: 84.88245\n",
      "Loss: 46.786957\n",
      "Loss: 221.25635\n",
      "Loss: 80.600525\n",
      "Loss: 47.646824\n",
      "Loss: 80.658554\n",
      "Loss: 133.14206\n",
      "Loss: 38.80171\n",
      "Loss: 68.400055\n",
      "Loss: 119.239525\n",
      "Loss: 75.19002\n",
      "Loss: 80.298256\n",
      "Loss: 46.840652\n",
      "Loss: 89.259476\n",
      "Loss: 85.79883\n",
      "Loss: 108.181274\n",
      "Loss: 110.957466\n",
      "Loss: 137.32675\n",
      "Loss: 100.30733\n",
      "Loss: 52.80013\n",
      "Loss: 33.30529\n",
      "Loss: 90.97588\n",
      "Loss: 66.37424\n",
      "Loss: 105.6944\n",
      "Loss: 25.201963\n",
      "Loss: 52.026848\n",
      "Loss: 115.495895\n",
      "Loss: 85.96664\n",
      "Loss: 92.94201\n",
      "Loss: 34.9952\n",
      "Loss: 131.6127\n",
      "Loss: 31.889507\n",
      "Loss: 93.80492\n",
      "Loss: 132.92883\n",
      "Loss: 120.321045\n",
      "Loss: 50.53984\n",
      "Loss: 56.542458\n",
      "Loss: 121.10282\n",
      "Loss: 72.479225\n",
      "Loss: 79.92617\n",
      "Loss: 105.00981\n",
      "Loss: 74.58396\n",
      "Loss: 87.32008\n",
      "Loss: 87.19834\n",
      "Loss: 69.84728\n",
      "Loss: 86.359276\n",
      "Loss: 160.2831\n",
      "Loss: 62.92264\n",
      "Loss: 124.19446\n",
      "Loss: 101.26145\n",
      "Loss: 64.333015\n",
      "Loss: 73.89307\n",
      "Loss: 42.36621\n",
      "Loss: 87.12672\n",
      "Loss: 109.871185\n",
      "Loss: 116.77636\n",
      "Loss: 95.64516\n",
      "Loss: 34.949604\n",
      "Loss: 88.78366\n",
      "Loss: 106.774086\n",
      "Loss: 104.68133\n",
      "Loss: 35.657444\n",
      "Loss: 68.83613\n",
      "Loss: 69.046715\n",
      "Loss: 58.507458\n",
      "Loss: 99.767426\n",
      "Loss: 79.56819\n",
      "Loss: 112.670105\n",
      "Loss: 76.08463\n",
      "Loss: 130.6117\n",
      "Loss: 78.051926\n",
      "Loss: 83.20258\n",
      "Loss: 63.236786\n",
      "Loss: 69.02753\n",
      "Loss: 71.63818\n",
      "Loss: 124.964745\n",
      "Loss: 89.45999\n",
      "Loss: 62.803024\n",
      "Loss: 65.6836\n",
      "Loss: 110.262405\n",
      "Loss: 41.91833\n",
      "Loss: 55.593315\n",
      "Loss: 55.45399\n",
      "Loss: 72.357605\n",
      "Loss: 114.055275\n",
      "Loss: 75.06818\n",
      "Loss: 72.80535\n",
      "Loss: 45.336815\n",
      "Loss: 87.275085\n",
      "Loss: 60.506855\n",
      "Loss: 117.99591\n",
      "Loss: 74.73177\n",
      "Loss: 96.83932\n",
      "Loss: 86.01899\n",
      "Loss: 38.95114\n",
      "Loss: 93.046234\n",
      "Loss: 47.687584\n",
      "Loss: 72.82558\n",
      "Loss: 100.48217\n",
      "Loss: 90.486595\n",
      "Loss: 106.277405\n",
      "Loss: 67.980606\n",
      "Loss: 89.67218\n",
      "Loss: 67.16838\n",
      "Loss: 113.629105\n",
      "Loss: 78.59434\n",
      "Loss: 85.53878\n",
      "Loss: 107.97833\n",
      "Loss: 62.00597\n",
      "Loss: 84.42904\n",
      "Loss: 63.11391\n",
      "Loss: 38.784885\n",
      "Loss: 73.793976\n",
      "Loss: 89.7666\n",
      "Loss: 111.42433\n",
      "Loss: 97.66149\n",
      "Loss: 98.16322\n",
      "Loss: 92.4296\n",
      "Loss: 44.5017\n",
      "Loss: 34.637646\n",
      "Loss: 89.54538\n",
      "Loss: 87.208725\n",
      "Loss: 100.87675\n",
      "Loss: 74.48299\n",
      "Loss: 29.257072\n",
      "Loss: 56.874146\n",
      "Loss: 91.1967\n",
      "Loss: 56.667297\n",
      "Loss: 71.12183\n",
      "Loss: 141.5863\n",
      "Loss: 62.314754\n",
      "Loss: 68.51283\n",
      "Loss: 41.88314\n",
      "Loss: 60.569386\n",
      "Loss: 103.45891\n",
      "Loss: 103.97639\n",
      "Loss: 120.59766\n",
      "Loss: 142.63948\n",
      "Loss: 68.1538\n",
      "Loss: 104.15661\n",
      "Loss: 105.88788\n",
      "Loss: 73.52547\n",
      "Loss: 108.25563\n",
      "Loss: 48.831497\n",
      "Loss: 59.14315\n",
      "Loss: 90.82615\n",
      "Loss: 82.27783\n",
      "Loss: 56.337082\n",
      "Loss: 25.00566\n",
      "Loss: 31.174797\n",
      "Loss: 97.297874\n",
      "Loss: 97.73616\n",
      "Loss: 35.322754\n",
      "Loss: 86.01625\n",
      "Loss: 93.94662\n",
      "Loss: 84.66313\n",
      "Loss: 166.62608\n",
      "Loss: 63.5371\n",
      "Loss: 77.843605\n",
      "Loss: 53.12673\n",
      "Loss: 145.8177\n",
      "Loss: 121.80749\n",
      "Loss: 73.970276\n",
      "Loss: 122.525185\n",
      "Loss: 86.181404\n",
      "Loss: 70.36441\n",
      "Loss: 67.39417\n",
      "Loss: 40.563694\n",
      "Loss: 68.89462\n",
      "Loss: 52.837444\n",
      "Loss: 107.87789\n",
      "Loss: 82.32368\n",
      "Loss: 107.94886\n",
      "Loss: 70.538185\n",
      "Loss: 102.49775\n",
      "Loss: 54.210014\n",
      "Loss: 73.33376\n",
      "Loss: 106.162445\n",
      "Loss: 53.460915\n",
      "Loss: 167.67448\n",
      "Loss: 40.59474\n",
      "Loss: 78.17895\n",
      "Loss: 87.21187\n",
      "Loss: 57.258736\n",
      "Loss: 73.53126\n",
      "Loss: 49.54889\n",
      "Loss: 116.178795\n",
      "Loss: 73.87211\n",
      "Loss: 75.44604\n",
      "Loss: 34.534286\n",
      "Loss: 82.96011\n",
      "Loss: 118.53966\n",
      "Loss: 121.27945\n",
      "Loss: 67.772545\n",
      "Loss: 78.2639\n",
      "Loss: 113.73938\n",
      "Loss: 50.925026\n",
      "Loss: 86.14179\n",
      "Loss: 69.48564\n",
      "Loss: 60.58075\n",
      "Loss: 131.98225\n",
      "Loss: 77.80827\n",
      "Loss: 84.78716\n",
      "Loss: 77.87425\n",
      "Loss: 79.052704\n",
      "Loss: 90.34181\n",
      "Loss: 105.25852\n",
      "Loss: 101.9161\n",
      "Loss: 50.02292\n",
      "Loss: 32.348602\n",
      "Loss: 93.85972\n",
      "Loss: 109.442505\n",
      "Loss: 99.751114\n",
      "Loss: 103.04552\n",
      "Loss: 90.98204\n",
      "Loss: 81.86881\n",
      "Loss: 101.951126\n",
      "Loss: 123.01461\n",
      "Loss: 151.10828\n",
      "Loss: 86.65796\n",
      "Loss: 97.73921\n",
      "Loss: 114.86473\n",
      "Loss: 98.05197\n",
      "Loss: 111.8676\n",
      "Loss: 28.758448\n",
      "Loss: 69.99706\n",
      "Loss: 67.009476\n",
      "Loss: 88.72981\n",
      "Loss: 62.34803\n",
      "Model saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-22212b1f41a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlatent_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./checkpoints/mnist.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_s = 1\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess,'./checkpoints/simple.ckpt')\n",
    "    try:\n",
    "        for i in range(epochs):\n",
    "            print('Epoch ' + str(i))\n",
    "            for b in range(42000):\n",
    "                sess.run(mnist_train,{X:train_x[b*batch_s:b*batch_s + batch_s],latent_samples:latent_dist[b*batch_s:b*batch_s + batch_s]})\n",
    "                if b % 100 == 0:\n",
    "                    saver.save(sess,'./checkpoints/mnist.ckpt')\n",
    "                    print(\"Loss: \" + str(sess.run(squared_loss,{X:train_x[b*batch_s:b*batch_s + batch_s],latent_samples:latent_dist[b*batch_s:b*batch_s + batch_s]})))\n",
    "    except Exception as e:\n",
    "        traceback.format_exc(e)\n",
    "    finally:\n",
    "        saver.save(sess,'./checkpoints/mnist.ckpt')\n",
    "        print('Model saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "Based on variational inference\n",
    "\n",
    "$$P(Z|X) = \\frac{P(Z,X)}{P(X)}$$\n",
    "\n",
    "## Some basic information theory\n",
    "### Information \n",
    "$I = -log(P(x))$\n",
    "* Measures the factor of uncertainty removed when x is known\n",
    "* 1 bit can be thought of as information that reduces uncertainty\n",
    "    by a factor of 2\n",
    "E.g. Let's say there is 50% chance of weather being sunny and \n",
    "50% chance of rain tomorrow. When the weather station tells us it is going to be sunny, they have given us 1 bit of information.\n",
    "\n",
    "> Uncertainty reduction is the inverse of the event's probability\n",
    "\n",
    "E.g. If the weather probabilities are sunny 75% and rain 25%,\n",
    "finding out that it is going to be rainy will reduce our \n",
    "uncertainty by $1/0.25 = 4$. This corresponds to $log_2(4) = 2$ bits of information $log_2(\\frac{1}{0.25}) = -log_2(0.25)$\n",
    "\n",
    "### Entropy \n",
    "$H = \\sum_x-log(P(x)) * P(x))$  \n",
    "* Can be thought of as average amount of information relayed by a certain distribution\n",
    "E.g in the above case, the weather station on average transmits\n",
    "$$ H = 0.75 \\times -log(0.75) + 0.25 \\times (-log(0.25)) \n",
    "  = 0.81$$ bits of useful information\n",
    "\n",
    "### Cross-entropy\n",
    "$$H(P,Q) = \\sum_x P(x)(-log(Q(x))$$\n",
    "\n",
    "E.g Lets say we use 2 bits to encode our weather prediction\n",
    "this can be thought of as us predicting the weather to have\n",
    "a 25% chance of either being sunny or rainy\n",
    "The average number of actual bits sent is\n",
    "$H = 0.75 \\times 2 + 0.25 \\times 2 = 2 $bits. If using different number of bits for the different predictions $H = 0.75 \\times 2 + 0.25 \\times 3 = 2.25 $bits \n",
    "\n",
    "Based on the entropy and cross-entropy, we can see that our _predicted_ probability distribution Q(x) differs from the _actual_ probability distribution P(x) by \n",
    "$KL(P||Q) = 2.25 - 0.81 = 1.54 $bits  \n",
    "If predictions are perfect i.e. Q(x) = P(x), then H(P,Q) = H(P)  \n",
    "Therefore, $H(P,Q) = H(P) + KL(P||Q)$  \n",
    "$KL(P||Q)$ means KL-divergence of Q w.r.t P\n",
    "\n",
    "\\begin{align}\n",
    "KL(P||Q) &= H(P,Q) - H(P)\\\\\n",
    "         &= \\sum_x P(x)(-log(Q(x)) -  \\sum_xP(x)(-log(P(x))\\\\\n",
    "         &= \\sum_{x} P(x)(-log(Q(x) - (-log(P(x)))\\\\\n",
    "         &= \\sum_x P(x)(log(P(x)) - log(Q(x)))\\\\\n",
    "         &= \\sum_x P(x)(log(\\frac{P(x)}{Q(x)}))\\\\\n",
    "\\end{align}\n",
    "#### Some properties of KL-divergence\n",
    "1. $KL(P||Q)$ is alwaysgreater than or equal to 0\n",
    "2. $KL(P||Q)$ is not the same as $KL(Q||P)$\n",
    "\n",
    "\n",
    "## Variational Bayes\n",
    "\n",
    "$$P(Z|X) = \\frac{P(Z,X)}{P(X)} = \\frac{P(X|Z)P(Z)}{P(X)}$$\n",
    "\n",
    "We don't know P(X). If we were to compute it,\n",
    "$P(X) = \\int{P(X|Z)P(Z)dZ}$\n",
    "* Intractable in many cases\n",
    "* If distributions are high dimensional, integral is multi-integral\n",
    "\n",
    "Thus, we can try to approximate the distribution. One method to approximate is Monte Carlo method (Gibbs sampling and other sampling methods) which is unbiased with high variance.\n",
    "\n",
    "Another is variational inference which has low variance but is biased\n",
    "\n",
    "1. Approximate P(Z|X) with Q(Z) that is tractable e.g. Gaussian\n",
    "2. Play with the parameters of Q(Z) in a way that it gets close enough to P(Z|X) i.e. minimize $KL(Q(Z)||P(Z|X))$\n",
    "\n",
    "This brings us to the following objective of minimizing \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "KL(Q(Z)||P(Z|X)) &= \\sum_z Q(Z)log(\\frac{Q(Z)}{P(Z|X)})\\\\\n",
    "                 &= - \\sum_zQ(Z) log(\\frac{P(Z|X)}{Q(Z)})\\\\\n",
    "                 &= - \\sum_z Q(Z) log(\\frac{P(X,Z)}{P(X) Q(Z)})\\\\\n",
    "                 &= - \\sum_z Q(Z) (log(\\frac{P(X,Z)}{Q(Z)}) - log(P(X)))\\\\\n",
    "                 &= - \\sum_z Q(Z) log(\\frac{P(X,Z)}{Q(Z)}) + log(P(X))\\\\\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\therefore log(P(X))    &= KL(Q(Z)||P(Z|X)) + \\sum_z Q(Z)log(\\frac{P(X,Z)}{Q(Z)})\\\\\n",
    "                 &= KL(Q(Z)||P(Z|X)) + L\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "As $log(P(X))$ is a constant, to minimize $KL(Q(Z)||P(Z|X))$,\n",
    "we just need to maximize $L$.\n",
    "\n",
    "$$\\because KL(Q(Z)||P(Z|X)) \\geq 0$$,  \n",
    "$$L \\leq P(X)$$ Thus, L is a lower bound of P(X).\n",
    "\n",
    "\\begin{align}\n",
    "L &= \\sum_z Q(Z) log(\\frac{P(X,Z)}{Q(Z)})\\\\\n",
    "  &= \\sum_z Q(Z) log(\\frac{P(X|Z)P(Z)}{Q(Z)})\\\\\n",
    "  &= \\sum_z Q(Z)(log(P(X|Z)) + log(\\frac{P(Z)}{Q(Z)}))\\\\\n",
    "  &= \\sum_z Q(Z) log(P(X|Z))) + \\sum_z Q(Z) log(\\frac{P(Z)}{Q(Z)})\\\\\n",
    "\\end{align}\n",
    "\n",
    "$$\\sum_z Q(Z) log(P(X|Z))) = E_{Q(Z)}P(X|Z)$$\n",
    "$$\\sum_z Q(Z) log(\\frac{P(Z)}{Q(Z)}) = -KL(Q(Z)||P(Z))$$\n",
    "Representing L as an autoencoder\n",
    "\n",
    "X --> Q(Z|X) --> Z --> P(X|Z) --> X'\n",
    "\n",
    "$E_{Q(Z)}P(X|Z)$ term acts as reconstruction error.\n",
    "$P(X|Z)$ is deterministic meaning one input will get the same output all the time. Thus, it can be considered $P(X|X')$.\n",
    "\n",
    "If $P(X|X')$ is gaussian\n",
    "$$P(X|X') = e^{-|X - X'|^2}$$\n",
    "$$log(P(X|X')) = -|X - X'|^2$$ --> L2 loss\n",
    "\n",
    "If Bernoulli distribution, will be similar to cross-entropy\n",
    "\n",
    "So far the network is all deterministic\n",
    "> To make it probabilistic, \n",
    "    encoder should not parametrize Z but instead the parametrize\n",
    "    the distribution that generates Z i.e. $\\mu$ and $\\sigma$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../train.csv')\n",
    "labels = np.asarray(df.label).reshape(42000,1)\n",
    "train_x = np.asarray(df[df.columns[1:]]).reshape(42000,28*28)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ SIMPLE MODEL ###############################################\n",
    "mu_data, sigma_data = 3., 1.75\n",
    "mu_z, sigma_z = 0., 1.\n",
    "alpha = 0.2\n",
    "np.random.seed(0)\n",
    "#data_distribution = np.random.normal(mu_data, sigma_data, (40000, 10))\n",
    "#data_distribution /= np.max(data_distribution,axis=1).reshape(40000,1)\n",
    "\n",
    "latent_distribution = np.random.normal(mu_z, sigma_z, (42000, 16))\n",
    "tf.Graph().as_default()\n",
    "X = tf.placeholder(tf.float32, (None, 784))\n",
    "with tf.name_scope('encoder'):\n",
    "    en_1 = tf.layers.BatchNormalization()(X)\n",
    "    en_1 = tf.layers.Dense(512)(en_1)\n",
    "    en_1 = tf.maximum(en_1, alpha*en_1)\n",
    "  #  en_2 = tf.layers.Dense(256)(en_1)\n",
    "  #  en_2 = tf.maximum(en_2, alpha*en_2)\n",
    "  #  en_3 = tf.layers.Dense(128)(en_2)\n",
    "  #  en_3 = tf.maximum(en_3, alpha*en_3)\n",
    "  #  encoded = tf.layers.Dense(32)(en_3)\n",
    "    #encoded = tf.maximum(encoded, alpha*encoded)\n",
    "    mean = tf.layers.Dense(16)(en_1)\n",
    "    log_stdev = tf.layers.Dense(16)(en_1)\n",
    "#log_stdev = encoded[:,0:16]\n",
    "#mean = encoded[:,16:32]\n",
    "latent_samples = tf.placeholder(tf.float32, (None,16))\n",
    "Z = latent_samples * tf.exp(log_stdev) + mean\n",
    "with tf.name_scope('decoder'):\n",
    "    de_3 = tf.layers.BatchNormalization()(Z)\n",
    " #   de_1 = tf.layers.Dense(128)(Z)\n",
    " #   de_1 = tf.maximum(de_1, alpha*de_1)\n",
    " #   de_2 = tf.layers.Dense(256)(de_1)\n",
    " #   de_2 = tf.maximum(de_2, alpha*de_2)\n",
    "    de_3 = tf.layers.Dense(512)(de_3)\n",
    "    de_3 = tf.maximum(de_3, alpha*de_3)\n",
    "    output = tf.layers.Dense(784,activation=tf.nn.sigmoid)(de_3)\n",
    "scale = 1.\n",
    "beta = 1.3\n",
    "recon_loss = tf.reduce_sum((tf.square(X - output)),axis=1)#tf.keras.losses.binary_crossentropy(X,output) #\n",
    "kl_loss = - 0.5 * tf.reduce_sum(1 + log_stdev - tf.square(mean) - tf.square(tf.exp(log_stdev)), axis=1)\n",
    "loss = tf.reduce_mean(scale*recon_loss + beta*kl_loss)\n",
    "adam = tf.train.AdamOptimizer()\n",
    "train = adam.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "bs = 100\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess,'./checkpoints/simple.ckpt')\n",
    "    try:\n",
    "        for i in range(epochs):\n",
    "            print('Ep: ' + str(i))\n",
    "            #plt.imshow(sess.run(output,{X:train_x[0:1],latent_samples:latent_distribution[0:1]})[0].reshape((28,28)))\n",
    "            for b in range(420):\n",
    "            #batch = train_x[b*batch_s:]\n",
    "                sess.run(train,{X:train_x[b*bs:b*bs+1],latent_samples:latent_distribution[b*bs:b*bs+1]})\n",
    "                if b % 1 == 0:\n",
    "                    saver.save(sess,'./checkpoints/simple.ckpt')\n",
    "                    print(\"Loss: \" + str(sess.run(loss,{X:train_x[b*bs:b*bs+1],latent_samples:latent_distribution[b*bs:b*bs+1]})))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        saver.save(sess,'./checkpoints/simple.ckpt')\n",
    "        print('Model saved')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess,'./checkpoints/simple.ckpt')\n",
    "    #print(data_distribution[0:5])\n",
    "    #print(sess.run(output,{X:data_distribution,latent_samples:latent_distribution})[0:5])\n",
    "    #print(sess.run(loss,{X:data_distribution,latent_samples:latent_distribution}))\n",
    "    plt.imshow(sess.run(output,{X:train_x[1].reshape([-1,784]),latent_samples:latent_distribution[1].reshape([-1,16])})[0].reshape((28,28)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_x[2].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### MNIST MODEL ########################################\n",
    "k = 9\n",
    "mu_z, sigma_z = 0., 1.\n",
    "latent_dist = np.random.normal(mu_z,sigma_z,(42000,7,7,32))\n",
    "tf.Graph().as_default()\n",
    "X = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "with tf.name_scope('encoder'):\n",
    "    en1 = tf.layers.Conv2D(64, (k,k))(X)\n",
    "    en1 = tf.nn.leaky_relu(en1)\n",
    "    en2 = tf.layers.Conv2D(64, (k,k), strides=(2,2))(en1)\n",
    "    en2 = tf.nn.leaky_relu(en2)\n",
    "    en3 = tf.layers.Conv2D(64, (3,3), strides=(1,1))(en2)\n",
    "    en3 = tf.nn.leaky_relu(en3)\n",
    "    en4 = tf.layers.Conv2D(64, (3,3), strides=(1,1))(en3)\n",
    "    en4 = tf.nn.leaky_relu(en3)\n",
    "    encoded = tf.layers.Conv2D(32, (3,3),padding='same',strides=(2,2))(en2)\n",
    "    #encoded = tf.nn.leaky_relu(encoded)\n",
    "batch_size = tf.shape(encoded)[0]\n",
    "latent_samples = tf.placeholder(tf.float32,(None,7,7,32))\n",
    "log_sigma = encoded[:,:,:,0:32]\n",
    "mu = encoded[:,:,:,32:64]\n",
    "Z = tf.exp(log_sigma) * latent_samples + mu \n",
    "\n",
    "with tf.name_scope('decoder'):\n",
    "    de_3 = tf.layers.Dense(512)(Z)\n",
    "    de_3 = tf.maximum(de_3, alpha*de_3)\n",
    "    output = tf.layers.Dense(784,activation=tf.nn.sigmoid)(de_3)\n",
    "'''\n",
    "with tf.name_scope('decoder'):\n",
    "    de1 = tf.layers.Conv2DTranspose(64,(3,3),padding='same',strides=(2,2))(Z)\n",
    "    de1 = tf.nn.leaky_relu(de1)\n",
    "    de2 = tf.layers.Conv2DTranspose(64,(7,7))(de1)\n",
    "    de2 = tf.nn.leaky_relu(de2)\n",
    "    de3 = tf.layers.Conv2DTranspose(64,(9,9))(de2)\n",
    "    de3 = tf.nn.leaky_relu(de3)\n",
    "    output = tf.layers.Conv2DTranspose(1,(3,3),padding='same',activation=tf.nn.sigmoid)(de3)\n",
    "'''\n",
    "beta = 1.3\n",
    "squared_loss = tf.reduce_sum(tf.)#tf.reduce_sum(tf.square(X - output))\n",
    "kl = beta * tf.reduce_sum(tf.square(tf.exp(log_sigma)) + tf.square(mu) - 1. - log_sigma)\n",
    "mnist_train = tf.train.AdamOptimizer().minimize(squared_loss+kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_s = 1\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver.restore(sess,'./checkpoints/simple.ckpt')\n",
    "    try:\n",
    "        for i in range(epochs):\n",
    "            print('Epoch ' + str(i))\n",
    "            for b in range(42000):\n",
    "                sess.run(mnist_train,{X:train_x[b*batch_s:b*batch_s + batch_s],latent_samples:latent_dist[b*batch_s:b*batch_s + batch_s]})\n",
    "                if b % 100 == 0:\n",
    "                    saver.save(sess,'./checkpoints/mnist.ckpt')\n",
    "                    print(\"Loss: \" + str(sess.run(squared_loss,{X:train_x[b*batch_s:b*batch_s + batch_s],latent_samples:latent_dist[b*batch_s:b*batch_s + batch_s]})))\n",
    "    except Exception as e:\n",
    "        traceback.format_exc(e)\n",
    "    finally:\n",
    "        saver.save(sess,'./checkpoints/mnist.ckpt')\n",
    "        print('Model saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
